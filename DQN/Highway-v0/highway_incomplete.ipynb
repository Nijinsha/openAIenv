{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.622222222222224\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "#     time.sleep(0.001)\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done,_ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.observation_space.high\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array(env.observation_space.sample())\n",
    "state = torch.from_numpy(state).float().float().unsqueeze(0)\n",
    "state = state.view(-1, 5*5)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 5*5\n",
    "action_size = env.action_space.n\n",
    "action_size, state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 1.0\n",
    "UPDATE_EVERY = 4\n",
    "TAU = 1e-3\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(30)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, action_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = self.dropout(state)\n",
    "        state = F.relu(self.fc2(state))\n",
    "        state = self.dropout(state)\n",
    "        \n",
    "        action = F.softmax(self.fc3(state), dim=1)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.seed = random.seed(30)\n",
    "        \n",
    "        self.memory = deque(maxlen=BUFFER_SIZE)\n",
    "        self.experiences = namedtuple(\"Experiences\", field_names=[\"state\",\"action\",\"reward\",\"new_state\",\"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        e = self.experiences(state, action, reward, new_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        new_states = torch.from_numpy(np.vstack([e.new_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return states, actions, rewards, new_states, dones\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.seed = random.seed(30)\n",
    "        self.action_size = action_size\n",
    "        self.local_nn = Model(state_size, action_size).to(device)\n",
    "        self.target_nn = Model(state_size, action_size).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.local_nn.parameters(), lr=0.01)\n",
    "        \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, new_state, done):\n",
    "        state = torch.from_numpy(state).float().float().unsqueeze(0)\n",
    "        state = state.view(-1, 5*5)\n",
    "        \n",
    "        new_state = torch.from_numpy(new_state).float().unsqueeze(0)\n",
    "        new_state = new_state.view(-1, 5*5)\n",
    "        \n",
    "        self.memory.add(state, action, reward, new_state, done)\n",
    "        \n",
    "        self.t_step += self.t_step % UPDATE_EVERY\n",
    "        \n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(BATCH_SIZE)\n",
    "                self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def act(self, state, eps):\n",
    "        state = torch.from_numpy(state).float().float().unsqueeze(0)\n",
    "        state = state.view(-1, 5*5)\n",
    "        \n",
    "        self.local_nn.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_nn(state)\n",
    "        self.local_nn.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, new_states, dones = experiences\n",
    "        \n",
    "        q_target = self.target_nn(new_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_target_value = rewards + gamma*q_target*(1-dones)\n",
    "        \n",
    "        q_expected = self.local_nn(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(q_expected, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.local_nn, self.target_nn, TAU)\n",
    "        \n",
    "    def soft_update(self,local_nn, target_nn, tau):\n",
    "        for target_param, local_param in zip(target_nn.parameters(), local_nn.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn():\n",
    "    n_episodes = 1000\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    eps = 1.0\n",
    "    min_eps = 0.005\n",
    "    decay = 0.995\n",
    "    max_score = -np.Inf\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0 \n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.act(state, eps)\n",
    "            print('\\r action {}'.format(action), end=\"\")\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, new_state, done)\n",
    "            state = new_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        eps = max(eps*decay, min_eps)\n",
    "        \n",
    "        print('\\rEpisode: {}\\t Score: {}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % 10 == 0 and i_episode>9:\n",
    "            if np.mean(scores_deque) > max_score:\n",
    "                max_score = np.mean(scores_deque)\n",
    "                torch.save(agent.local_nn.state_dict(), 'checkPoint.pth')\n",
    "            print('\\rEpisode: {}\\t Score: {}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sans/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\t Score: 22.391919191919197\n",
      "Episode: 20\t Score: 20.077248677248683\n",
      "Episode: 30\t Score: 19.487455197132622\n",
      "Episode: 40\t Score: 21.401626016260174\n",
      "Episode: 50\t Score: 20.282352941176477\n",
      "Episode: 60\t Score: 20.333697632058293\n",
      "Episode: 70\t Score: 20.415962441314563\n",
      "Episode: 80\t Score: 20.249108367626892\n",
      "Episode: 90\t Score: 20.911599511599527\n",
      "Episode: 100\t Score: 21.17822222222223\n",
      "Episode: 110\t Score: 21.152222222222225\n",
      "Episode: 120\t Score: 21.645333333333344\n",
      "Episode: 130\t Score: 22.544666666666675\n",
      "Episode: 140\t Score: 22.295555555555566\n",
      "Episode: 150\t Score: 23.423111111111126\n",
      "Episode: 160\t Score: 24.105555555555567\n",
      "Episode: 170\t Score: 23.929555555555567\n",
      "Episode: 180\t Score: 24.913333333333345\n",
      "Episode: 190\t Score: 24.403111111111125\n",
      "Episode: 200\t Score: 24.336000000000014\n",
      "Episode: 210\t Score: 24.747333333333348\n",
      "Episode: 220\t Score: 24.253111111111122\n",
      "Episode: 230\t Score: 24.189777777777785\n",
      "Episode: 240\t Score: 23.878444444444447\n",
      "Episode: 250\t Score: 23.905777777777798\n",
      " action 0251\t Score: 23.9208888888889"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-b9f8c29857a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-237-0d0fb72961d3>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r action {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/highway_env/envs/highway_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/highway_env/envs/common/abstract.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The road and vehicle must be initialized in the environment implementation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/highway_env/envs/common/abstract.py\u001b[0m in \u001b[0;36m_simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIMULATION_FREQUENCY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/highway_env/road/road.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvehicles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/highway_env/vehicle/behavior.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     80\u001b[0m                                                    rear_vehicle=rear_vehicle)\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# action['acceleration'] = self.recover_from_stop(action['acceleration'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acceleration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acceleration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_MAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_MAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \"\"\"\n\u001b[0;32m-> 2084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112022100128.31111111111111\n"
     ]
    }
   ],
   "source": [
    "agent.local_nn.load_state_dict(torch.load('checkPoint.pth'))\n",
    "agent.local_nn.eval()\n",
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "#     time.sleep(0.001)\n",
    "    env.render()\n",
    "    state = torch.from_numpy(state).float().float().unsqueeze(0)\n",
    "    state = state.view(-1, 5*5)\n",
    "    \n",
    "    action_values = agent.local_nn(state)\n",
    "    action = random.choice(np.arange(3))\n",
    "    print(action, end=\"\")\n",
    "    state, reward, done,_ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
